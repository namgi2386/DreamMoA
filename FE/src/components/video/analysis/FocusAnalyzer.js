import { useEffect, useRef } from "react";
import * as poseDetection from "@tensorflow-models/pose-detection";
import * as cocoSsd from "@tensorflow-models/coco-ssd";

const FocusAnalysis = ({ videoRef, socket }) => {
    const detectorRef = useRef(null);  // ğŸ”¥ Mediapipe í¬ì¦ˆ ê°ì§€ ëª¨ë¸
    const yoloModelRef = useRef(null); // ğŸ”¥ YOLO íœ´ëŒ€í° ê°ì§€ ëª¨ë¸

    useEffect(() => {
        // âœ… Mediapipe & YOLO ëª¨ë¸ ë¡œë“œ
        const loadModels = async () => {
            detectorRef.current = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet);
            yoloModelRef.current = await cocoSsd.load();
        };
        loadModels();

        // âœ… 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ìº¡ì²˜ ë° ë°ì´í„° ì „ì†¡
        const captureFrames = async () => {
            if (!videoRef.current || !detectorRef.current || !yoloModelRef.current) return;

            let frameData = [];
            let phoneDetected = 0; // ğŸ”¥ í•œ ë²ˆì´ë¼ë„ ê°ì§€ë˜ë©´ 1ë¡œ ì„¤ì •

            for (let i = 0; i < 30; i++) { // ğŸ”¥ 30 í”„ë ˆì„(ëŒ€ëµ 1ì´ˆ ë™ì•ˆ) ì €ì¥
                const pose = await detectorRef.current.estimatePoses(videoRef.current.video);
                const objects = await yoloModelRef.current.detect(videoRef.current.video);

                const frame = {
                    frame_index: i,
                    head_x: pose[0]?.keypoints[0]?.x || 0,
                    head_y: pose[0]?.keypoints[0]?.y || 0,
                    neck_x: ((pose[0]?.keypoints[5]?.x || 0) + (pose[0]?.keypoints[6]?.x || 0)) / 2,
                    neck_y: ((pose[0]?.keypoints[5]?.y || 0) + (pose[0]?.keypoints[6]?.y || 0)) / 2,
                    shoulder_left_x: pose[0]?.keypoints[5]?.x || 0,
                    shoulder_left_y: pose[0]?.keypoints[5]?.y || 0,
                    shoulder_right_x: pose[0]?.keypoints[6]?.x || 0,
                    shoulder_right_y: pose[0]?.keypoints[6]?.y || 0,
                    wrist_left_x: pose[0]?.keypoints[9]?.x || 0,
                    wrist_left_y: pose[0]?.keypoints[9]?.y || 0,
                    wrist_right_x: pose[0]?.keypoints[10]?.x || 0,
                    wrist_right_y: pose[0]?.keypoints[10]?.y || 0,
                    head_tilt: pose[0]?.keypoints[0]?.z || 0,
                    eye_direction: ((pose[0]?.keypoints[1]?.x || 0) + (pose[0]?.keypoints[2]?.x || 0)) / 2,
                    phone_detected: objects.some((obj) => obj.class === "cell phone") ? 1 : 0
                };

                if (frame.phone_detected === 1) {
                    phoneDetected = 1;
                }

                frameData.push(frame);
                await new Promise(r => setTimeout(r, 33)); // ğŸ”¥ 30fps ê¸°ì¤€ ì•½ 33ms ëŒ€ê¸°
            }

            // âœ… WebSocket ì „ì†¡ ë°ì´í„°
            const payload = {
                timestamp: Date.now(),
                frame_data: frameData, // ğŸ”¥ 30í”„ë ˆì„ ì „ì²´ í¬í•¨
                phone_detected_percentage: phoneDetected, // ğŸ”¥ 1ì´ˆ ë™ì•ˆ í•œ ë²ˆì´ë¼ë„ ê°ì§€ë˜ë©´ 1
                head_tilt: frameData[frameData.length - 1].head_tilt, // ğŸ”¥ ë§ˆì§€ë§‰ í”„ë ˆì„ ê°’
                eye_direction: frameData[frameData.length - 1].eye_direction // ğŸ”¥ ë§ˆì§€ë§‰ í”„ë ˆì„ ê°’
            };

            console.log("ğŸ“¡ ë³´ë‚¼ ë°ì´í„°:", payload);
            socket.emit("send_focus_data", payload);
        };

        // âœ… 1ì´ˆë§ˆë‹¤ ì‹¤í–‰
        const interval = setInterval(captureFrames, 1000);
        
        return () => clearInterval(interval); // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    }, [videoRef, socket]);

    return null;
};

export default FocusAnalysis;
